# -*- coding: utf-8 -*-
"""Jaya Jaya Maju Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfoI8ctUu-dIkjtWhk1OcJPJ2-R21iLw

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

- Nama: Muhammad Ario Winaya
- Email: muhammad21121@mail.unpad.ac.id
- Id Dicoding: muhammad_ario_winaya

## Persiapan

### Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px # Data visualization
from sklearn.preprocessing import LabelEncoder # Data encoding
from scipy.stats.mstats import winsorize # Handle Outliers
from scipy import stats # Statistical test
import statsmodels.api as sm # Logistic Regression (Statistical test)
from sklearn.model_selection import train_test_split # Split data
from sklearn.preprocessing import RobustScaler # Standardization
from imblearn.over_sampling import SMOTE # Handle Imbalance Data

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve

from google.colab import drive
drive.mount('/content/drive')

import numpy
import pandas
import matplotlib
import seaborn
import sklearn
import statsmodels
import plotly
import scipy
import imblearn
from google.colab import drive
drive.mount('/content/drive')

"""### Menyiapkan data yang akan diguankan

## Data Understanding
"""

employee = pd.read_csv('/content/drive/MyDrive/Project Data Analysis/Dicoding Project/employee_data.csv')
employee.head()

"""### Deskripsi kolom dataset

- **EmployeeId**: Employee Identifier
- **Attrition**: Did the employee attrition? (0=no, 1=yes)
- **Age**: Age of the employee
- **BusinessTravel**: Travel commitments for the job
- **DailyRate**: Daily salary
- **Department**: Employee Department
- **DistanceFromHome**: Distance from work to home (in km)
- **Education**: 1-Below College, 2-College, 3-Bachelor, 4-Master, 5-Doctor
- **EducationField**: Field of Education
- **EnvironmentSatisfaction**: 1-Low, 2-Medium, 3-High, 4-Very High
- **Gender**: Employee's gender
- **HourlyRate**: Hourly salary
- **JobInvolvement**: 1-Low, 2-Medium, 3-High, 4-Very High
- **JobLevel**: Level of job (1 to 5)
- **JobRole**: Job Roles
- **JobSatisfaction**: 1-Low, 2-Medium, 3-High, 4-Very High
- **MaritalStatus**: Marital Status
- **MonthlyIncome**: Monthly salary
- **MonthlyRate**: Monthly rate
- **NumCompaniesWorked**: Number of companies worked at
- **Over18**: Over 18 years of age? (Yes/No)
- **OverTime**: Overtime? (Yes/No)
- **PercentSalaryHike**: The percentage increase in salary last year
- **PerformanceRating**: 1-Low, 2-Good, 3-Excellent, 4-Outstanding
- **RelationshipSatisfaction**: 1-Low, 2-Medium, 3-High, 4-Very High
- **StandardHours**: Standard Hours
- **StockOptionLevel**: Stock Option Level
- **TotalWorkingYears**: Total years worked
- **TrainingTimesLastYear**: Number of training attended last year
- **WorkLifeBalance**: 1-Low, 2-Good, 3-Excellent, 4-Outstanding
- **YearsAtCompany**: Years at Company
- **YearsInCurrentRole**: Years in the current role
- **YearsSinceLastPromotion**: Years since the last promotion
- **YearsWithCurrManager**: Years with the current manager

## Data Preparation / Preprocessing

### Memeriks missing value
"""

df = employee.copy()
df.info()

# Drop missing value
df_drop = df.dropna()
df_drop.info()

"""Terdapat data yang hilang pada variabel *Attrition*. Oleh karena itu, akan dilakukan penghapusan baris yang mengandung data hilang tersebut.

### Menghitung jumlah anggota tiap kelas
"""

df_drop['Attrition'].value_counts()

total_instances = 879 + 179
no_attrition_percentage = (879 / total_instances) * 100
attrition_percentage = (179 / total_instances) * 100

print("No Attrition Percentage: {:.2f}%".format(no_attrition_percentage))
print("Attrition Percentage: {:.2f}%".format(attrition_percentage))

"""Persentase kelas 'No Attrition' yang jauh lebih tinggi menunjukkan adanya ketidakseimbangan data yang signifikan. Oleh karena itu, diperlukan langkah-langkah khusus untuk mengatasi masalah ini.

### Pemeriksaan duplikasi data
"""

df_drop.duplicated().sum()

dashboard = df_drop.copy()

# Mapping dictionaries
attrition_map = {0: 'No Attrition', 1: 'Attrition'}
overtime_map = {'Yes': 'Overtime', 'No': 'Regular Hours'}
education_map = {1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Master', 5: 'Doctor'}
env_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
job_involvement_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
job_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
performance_rating_map = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}
relationship_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
work_life_balance_map = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}

# Apply mappings
dashboard['Attrition'] = dashboard['Attrition'].map(attrition_map)
dashboard['OverTime'] = dashboard['OverTime'].map(overtime_map)
dashboard['Education'] = dashboard['Education'].map(education_map)
dashboard['EnvironmentSatisfaction'] = dashboard['EnvironmentSatisfaction'].map(env_satisfaction_map)
dashboard['JobInvolvement'] = dashboard['JobInvolvement'].map(job_involvement_map)
dashboard['JobSatisfaction'] = dashboard['JobSatisfaction'].map(job_satisfaction_map)
dashboard['PerformanceRating'] = dashboard['PerformanceRating'].map(performance_rating_map)
dashboard['RelationshipSatisfaction'] = dashboard['RelationshipSatisfaction'].map(relationship_satisfaction_map)
dashboard['WorkLifeBalance'] = dashboard['WorkLifeBalance'].map(work_life_balance_map)

# Display the updated dataframe
dashboard

dashboard.to_csv('dashboard.csv', index=False)

"""Tidak terdapat data duplikat pada dataset

### Memisahkan kolom numerik dan kategorik

Pemisahan ini dilakukan untuk melaksanakan proses exploratory data analysis (EDA) yang sesuai dengan tipe data masing-masing variabel.
"""

data = df_drop.copy().drop(columns=['EmployeeId'])

categorical_columns = []
numerical_columns = []

for column in data.columns:
    if data[column].nunique() < 7 or data[column].dtypes == 'object':
        categorical_columns.append(column)
    else:
        numerical_columns.append(column)

print("Categorical Columns:")
for i, column in enumerate(categorical_columns, start=1):
    print(f"{i}. {column}")

print("\nNumerical Columns:")
for i, column in enumerate(numerical_columns, start=1):
    print(f"{i}. {column}")

"""# Exploratory Data Analysis

Categorical Data
"""

cat = pd.DataFrame(categorical_columns)
cat.columns = ['Categorical Column']
cat.index = cat.index + 1
cat

# Change data Attrition to int
data['Attrition'] = data['Attrition'].astype(int)

# Descriptive statistics of categorical columns
round(data[categorical_columns].describe(),2)

for i, column in enumerate(categorical_columns, start=1):
    print(f"{i}. {column}")
    value_counts_df = pd.DataFrame(df[column].value_counts())
    value_counts_df.columns = ['count']
    print(value_counts_df)
    print("\n")

# Visualize data for every categorical columns
import plotly.express as px
for i, column in enumerate(categorical_columns, start=1):
    value_counts_df = pd.DataFrame(data[column].value_counts())
    value_counts_df.columns = ['count']
    if data[column].nunique() <= 4:
        fig = px.pie(value_counts_df, values='count', names=value_counts_df.index, title=f'{column} Distribution')
    else:
        fig = px.bar(value_counts_df, x=value_counts_df.index, y='count', title=f'{column} Distribution',
                     labels={'x': column, 'y': 'Count'})

    fig.show()

"""Berdasarkan visualisasi dari tiap kolom di atas, terlihat bahwa beberapa variabel hanya memiliki satu kategori. Oleh karena itu, variabel-variabel tersebut tidak akan digunakan dalam analisis.

Numerical Data
"""

num = pd.DataFrame(numerical_columns)
num.columns = ['Numerical Column']
num.index = num.index + 1
num

# Descriptive statistics of numerical column
round(data[numerical_columns].describe(),2)

# Visualize data for every numeric columns
for column in numerical_columns:
  fig = px.histogram(data, x=column, title=f'{column} Distribution')
  fig.show()

"""Berdasarkan visualisasi data di atas, terdapat beberapa variabel yang berdistribusi tidak normal, sehingga akan diterapkan penanganan pada outliers dan metode analisis yang lebih robust untuk memastikan hasil yang akurat."""

# Numerical correlation matrix
corr_matrix = data[numerical_columns].corr()

# Create a heatmap of the correlation matrix
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", ax=ax, cmap="coolwarm")
plt.show()

"""# Statistical Analysis

## Data encoding untuk variabel kategorik
"""

# Call categorical column with type object
cat_object = data[categorical_columns].select_dtypes(include=['object'])

# Drop column 'Over 18' because it only has one category
cat_object.drop(columns=['Over18'], inplace=True)

cat_object

# Data encoding
from sklearn.preprocessing import LabelEncoder
df_new = data.copy()
le = LabelEncoder()

for column in cat_object.columns:
    df_new[column] = le.fit_transform(df_new[column])

    # Print original and encoded values for comparison
    print(f"Column: {column}")
    print("Original Categories:", data[column].unique())
    print("Encoded Values:", df_new[column].unique())
    print("-" * 30)

# Drop column with that only has one category
df_new = df_new.drop(columns=['StandardHours','Over18','EmployeeCount'])

df_new.info()

"""## Penanganan outlier untuk variabel numerik"""

# Handling Outliers
from scipy.stats.mstats import winsorize

for column in numerical_columns:
    # Winsorize at the 5th and 95th percentiles
    df_new[column] = winsorize(df_new[column], limits=[0.05, 0.05])

for column in numerical_columns:
  fig = px.box(df_new, x=column, title=f'{column} Distribution')
  fig.show()

"""## Uji statistik untuk variabel kategorik"""

# Perform the Chi-square test
from scipy import stats

significant_cat = []
for column in categorical_columns:
    contingency_table = pd.crosstab(data[column], data['Attrition'])
    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
    print(f"Variable: {column}")
    print(f"Chi-square statistic: {chi2:.2f}")
    print(f"P-value: {p:.3f}\n")
    if p <= 0.05:
        significant_cat.append(column)
print("Categorical variables that have a significant effect on Attrition:")
significant_cat

"""**Uji Chi-Square** adalah teknik statistik yang digunakan untuk menentukan apakah ada hubungan yang signifikan antara dua variabel kategorikal yang terdapat dalam tabel kontingensi.

**Hipotesis Nol (H0) dan Hipotesis Alternatif (H1):**

H0: Tidak ada hubungan antara dua variabel (variabel independen dan variabel dependen).

H1: Terdapat hubungan antara dua variabel.

**Interpretasi dengan P-value:**

Jika nilai p (p-value) ≤ tingkat signifikansi (α) yang ditetapkan (α = 0.05), terdapat cukup bukti untuk menolak H0. Ini menunjukkan bahwa ada hubungan yang signifikan antara dua variabel yang diuji.

Berdasarkan hasil uji chi-square, variabel-variabel berikut memiliki pengaruh signifikan terhadap Attrition:

1. BusinessTravel
2. EnvironmentSatisfaction
3. JobInvolvement
4. JobLevel
5. JobRole
6. JobSatisfaction
7. MaritalStatus
8. OverTime
9. StockOptionLevel
10. WorkLifeBalance

Variabel-variabel signifikan ini selanjutnya akan digunakan untuk analisis regresi logistik

## Uji statistik untuk variabel numerik
"""

# Perform the Mann-Whitney U test
significant_num = []
for column in numerical_columns:
    mannw, p_value = stats.mannwhitneyu(df_new[column][df_new['Attrition'] == 0], df_new[column][df_new['Attrition'] == 1])
    print(f"Variable: {column}")
    print(f"Mann-Whitney U: {mannw:.2f}")
    print(f"P-value: {p_value:.3f}\n")
    if p_value < 0.05:
        significant_num.append(column)
print("Numerical variables that have a significant effect on Attrition:")
significant_num

"""**Uji Mann-Whitney** adalah teknik statistik non-parametrik yang digunakan untuk menentukan apakah terdapat perbedaan signifikan antara dua kelompok independen dalam sampel yang diberikan.

**Hipotesis Nol (H0) dan Hipotesis Alternatif (H1):**

H0: Tidak ada perbedaan signifikan antara dua kelompok.
H1: Terdapat perbedaan signifikan antara dua kelompok.

**Interpretasi dengan nilai p (p-value):**

Jika nilai p (p-value) ≤ tingkat signifikansi (α) yang ditetapkan (α = 0.05), maka terdapat cukup bukti untuk menolak H0. Ini menunjukkan bahwa ada perbedaan signifikan antara dua kelompok yang diuji.

Berdasarkan hasil uji Mann-Whitney, variabel-variabel berikut memiliki pengaruh signifikan terhadap Attrition:

1. Age
2. DistanceFromHome
3. MonthlyIncome
4. TotalWorkingYears
5. YearsAtCompany
6. YearsInCurrentRole
7. YearsWithCurrManager

Variabel-variabel signifikan ini selanjutnya akan digunakan untuk analisis regresi logistik

## Uji statistik dengan variabel numerik dan kategorik (**variabel signifikan**)
"""

# Logistic Regression with statmodels
independent_vars = significant_num + significant_cat
X = df_new[independent_vars].drop('Attrition', axis = 1)
y = df_new['Attrition']

# Split the data into training and testing sets (80% train, 20% test)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape, y_train.shape)
print("Testing set size:", X_test.shape, y_test.shape)

# Standardization of numerical variables
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()

# Scale and update numerical columns within X_train and X_test
X_train[significant_num] = scaler.fit_transform(X_train[significant_num])
X_test[significant_num] = scaler.transform(X_test[significant_num])

# Handle Imbalance Data
from imblearn.over_sampling import SMOTE

# Instantiate SMOTE
smote = SMOTE(random_state=14045)

# Resample the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Resampled training set shape:", X_train_resampled.shape, y_train_resampled.shape)

# Add a constant term to the independent variables
X_train_resampled = sm.add_constant(X_train_resampled)
X_test = sm.add_constant(X_test)

# Create the logistic regression model
model = sm.Logit(y_train_resampled, X_train_resampled)

# Fit the model
model_sig = model.fit()

# Print the model summary
print(model_sig.summary())

# List of significant variables
significant_vars = []
index = 1
for var, p_value in model_sig.pvalues.items():
    if p_value < 0.05:
        significant_vars.append(var)
        print(f"{index}. {var}: p-value = {p_value:.3f}")
        index += 1

"""# Modeling"""

independent_vars = significant_num + significant_cat
X = df_new[independent_vars].drop('Attrition', axis = 1)
y = df_new['Attrition']

# Split the data into training and testing sets (80% train, 20% test)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape, y_train.shape)
print("Testing set size:", X_test.shape, y_test.shape)

# Standardization of numerical variables
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()

# Scale and update numerical columns within X_train and X_test
X_train[significant_num] = scaler.fit_transform(X_train[significant_num])
X_test[significant_num] = scaler.transform(X_test[significant_num])

# Handle Imbalance Data
from imblearn.over_sampling import SMOTE

# Instantiate SMOTE
smote = SMOTE(random_state=14045)

# Resample the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Resampled training set shape:", X_train_resampled.shape, y_train_resampled.shape)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(kernel ="linear", probability =True),
    'Naive Bayes': GaussianNB()
}

"""# Evaluation"""

# Train, evaluate, and visualize results
results = []
for name, model in models.items():
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test)

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    if hasattr(model, "predict_proba"):
        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    else:
        roc_auc = "N/A"
    results.append([name, accuracy, precision, recall, f1, roc_auc])

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()

    # ROC curve
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_prob)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.show()

# Results DataFrame
results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC'])
print(results_df)

"""Output di atas adalah hasil evaluasi dari enam model machine learning. Karena dataset yang digunakan tidak seimbang (imbalance), maka model Regresi Logistik dipilih sebagai model terbaik karena menunjukkan keseimbangan yang baik untuk setiap metrik evaluasi."""

# Get Coefficients from Logistic Regression model
model = LogisticRegression()

# Fit the model
model.fit(X_train_resampled, y_train_resampled)

# Create a DataFrame for coefficients
coefficients = pd.DataFrame({
    'Variable': X.columns,
    'Coefficient': model.coef_[0]
})

print(coefficients)

importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.coef_[0]
})

# Sort by importance
importance = importance.sort_values('Importance', ascending=False)

# Create a bar plot with plotly express
fig = px.bar(importance, x='Feature', y='Importance',
             title='Feature Importance',
             labels={'Feature': 'Features', 'Importance': 'Importance'})

# Display the plot
fig.show()

"""# Interpretasi Koefisien Regresi Logistik: HR Analytics

Berdasarkan grafik di atas, dapat diinterpretasikan hubungan antara berbagai faktor karyawan dengan kemungkinan keluarnya karyawan dari perusahaan.

**Variabel Dependen:** Attrition (0 = Menetap, 1 = Keluar)

- **Koefisien Positif:** Meningkatkan peluang attrition (keluar)
- **Koefisien Negatif:** Menurunkan peluang attrition (tetap)

## Interpretasi per Variabel

1. **Age (Usia):**
   - Koefisien negatif (-0.279963) menunjukkan bahwa semakin tua usia karyawan, semakin kecil kemungkinannya untuk keluar dari perusahaan.

2. **DistanceFromHome (Jarak Rumah ke Tempat Kerja):**
   - Koefisien positif (0.291344) menunjukkan bahwa semakin jauh jarak rumah ke tempat kerja, semakin besar kemungkinannya untuk keluar dari perusahaan.

3. **MonthlyIncome (Gaji Bulanan):**
   - Koefisien positif (1.049603) menunjukkan bahwa semakin tinggi gaji bulanan, semakin besar kemungkinannya untuk keluar dari perusahaan. Ini mungkin counter-intuitive, namun bisa jadi karyawan dengan gaji tinggi mendapat tawaran menarik dari perusahaan lain.

4. **TotalWorkingYears (Total Tahun Bekerja):**
   - Koefisien negatif (-0.298440) menunjukkan bahwa semakin lama karyawan bekerja, semakin kecil kemungkinannya untuk keluar dari perusahaan.

5. **YearsAtCompany (Tahun di Perusahaan):**
   - Koefisien positif (0.656754) menunjukkan bahwa semakin lama karyawan berada di perusahaan, semakin besar kemungkinannya untuk keluar dari perusahaan. Ini mungkin perlu diinvestigasi lebih lanjut, apakah ada faktor tertentu yang menyebabkan karyawan senior keluar setelah lama bekerja.

6. **YearsInCurrentRole (Tahun di Jabatan Sekarang):**
   - Koefisien negatif (-0.236787) menunjukkan bahwa semakin lama karyawan berada di jabatan sekarang, semakin kecil kemungkinannya untuk keluar dari perusahaan.

7. **YearsWithCurrManager (Tahun dengan Manager Sekarang):**
   - Koefisien negatif (-0.872843) menunjukkan bahwa semakin lama karyawan bekerja dengan manager sekarang, semakin kecil kemungkinannya untuk keluar dari perusahaan. Hubungan yang kuat terindikasi dengan nilai koefisien absolut yang besar.

8. **BusinessTravel (Perjalanan Bisnis):**
   - Koefisien negatif (-0.484941) menunjukkan bahwa karyawan yang sering melakukan perjalanan bisnis lebih kecil kemungkinannya untuk keluar.

9. **EnvironmentSatisfaction (Kepuasan Lingkungan Kerja):**
   - Koefisien negatif (-0.531981) menunjukkan bahwa karyawan yang puas dengan lingkungan kerja lebih kecil kemungkinannya untuk keluar.

10. **JobInvolvement (Keterlibatan dalam Pekerjaan):**
    - Koefisien negatif (-0.809543) menunjukkan bahwa karyawan yang lebih terlibat dalam pekerjaan lebih kecil kemungkinannya untuk keluar. Hubungan yang kuat terindikasi dengan nilai koefisien absolut yang besar.

11. **JobLevel (Level Jabatan):**
    - Koefisien negatif (-1.159977) menunjukkan bahwa semakin tinggi level jabatan karyawan, semakin kecil kemungkinannya untuk keluar. Hubungan yang kuat terindikasi dengan nilai koefisien absolut yang besar.

12. **JobRole (Jabatan):**
    - Koefisien negatif (-0.004484) sangat mendekati nol, sehingga sulit disimpulkan. Mungkin perlu investigasi lebih lanjut.

13. **JobSatisfaction (Kepuasan Kerja):**
    - Koefisien negatif (-0.541815) menunjukkan bahwa karyawan yang puas dengan pekerjaan lebih kecil kemungkinannya untuk keluar.

14. **MaritalStatus (Status Pernikahan):**
    - Koefisien negatif (-0.253194) menunjukkan bahwa karyawan yang sudah menikah lebih kecil kemungkinannya untuk keluar.

15. **OverTime (Lembur):**
    - Koefisien positif (1.468370) menunjukkan bahwa karyawan yang sering lembur lebih besar kemungkinannya untuk keluar. Hubungan yang kuat terindikasi dengan nilai koefisien absolut yang besar.

16. **StockOptionLevel (Level Stock Option):**
    - Koefisien negatif (-1.394485) menunjukkan bahwa karyawan dengan level stock option yang lebih tinggi lebih kecil kemungkinannya untuk keluar. Hubungan yang kuat terindikasi dengan nilai koefisien absolut yang besar.

17. **WorkLifeBalance (Keseimbangan Hidup dan Kerja):**
    - Koefisien negatif (-0.614759) menunjukkan bahwa karyawan yang memiliki keseimbangan hidup dan kerja yang lebih baik lebih kecil kemungkinannya untuk keluar.

## Kesimpulan:

Tabel koefisien regresi logistik ini memberikan gambaran tentang berbagai faktor yang memengaruhi kemungkinan attrition karyawan.

**Faktor yang meningkatkan peluang attrition:**
- Jarak rumah ke tempat kerja
- Gaji Bulanan
- Tahun di Perusahaan
- OverTime

**Faktor yang menurunkan peluang attrition:**
- Usia
- Total Tahun Bekerja
- Tahun di Jabatan Sekarang
- Tahun dengan Manager Sekarang
- Perjalanan Bisnis
- Kepuasan Lingkungan Kerja
- Keterlibatan dalam Pekerjaan
- Level Jabatan
- Kepuasan Kerja
- Status Pernikahan
- Level Stock Option
- Keseimbangan Hidup dan Kerja

## Action Items:

- Meninjau kembali beban kerja dan memastikan distribusi tugas yang adil agar work life balnce karyawan terjaga.
- Meningkatkan kondisi lingkungan kerja fisik, seperti ruang kerja yang nyaman, fasilitas rekreasi, dan area istirahat yang memadai.
- Mendorong komunikasi yang terbuka dan transparan antara manajer dan karyawan.
- Melakukan peninjauan kebijakan stock option saat ini dan membandingkannya dengan praktik terbaik di industri untuk memastikan daya saing.

## Setup Database
"""

from sqlalchemy import create_engine

URL = "postgresql://postgres.axwpbvulqapkfdafulgk:winayaario678@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres"

engine = create_engine(URL)
df.to_sql('Attrition Data', engine)